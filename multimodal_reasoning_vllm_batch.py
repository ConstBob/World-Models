# -*- coding: utf-8 -*-
"""Multimodal_Reasoning_vLLM_batch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LsFORKjcwgr3UodMf8KpPyEfPB72mAW1
"""

!nvcc --version

# !pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830
!pip install git+https://github.com/huggingface/transformers
!pip install accelerate
!pip install qwen-vl-utils
!pip install datasets
!CUDA_VERSION=cu121
!pip install 'vllm==0.6.1' --extra-index-url https://download.pytorch.org/whl/${CUDA_VERSION}

from google.colab import drive
import os
import json
import re
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from datasets import load_dataset

# Mount Google Drive
drive.mount('/content/drive')

# Create necessary directories in Google Drive
output_dir = "/content/drive/My Drive/World Models"
os.makedirs(output_dir, exist_ok=True)

# Use GPU
if torch.cuda.is_available():
    print("GPU is available and ready to use!")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
else:
    print("GPU is not available. Make sure it's enabled in the runtime settings.")

# Response Generation with vLLM
from vllm import LLM, SamplingParams
from transformers import AutoProcessor
from qwen_vl_utils import process_vision_info

model_save_path = os.path.join(output_dir, "qwen_model")
if os.path.exists(model_save_path):
    print("Loading model from local save path.")
    model_path = model_save_path
else:
    print("Downloading model. This may take some time...")
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        "Qwen/Qwen2-VL-2B-Instruct", torch_dtype="auto", device_map="auto"
    )
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
    model.save_pretrained(model_save_path)
    processor.save_pretrained(model_save_path)

print(model_save_path)
print(model_path)

from transformers import AutoConfig

config = AutoConfig.from_pretrained(model_path)
if not hasattr(config, "rope_scaling") or "factor" not in config.rope_scaling:
    config.rope_scaling = {"type": "linear", "factor": 1.0}  # Default
if not hasattr(config, "rope_type"):
    config.rope_type = "yarn"  # Default
if not hasattr(config, "max_model_len"):
    config.max_model_len = 2048  # Adjust per the model

config.save_pretrained(model_path)

llm = LLM(
    model=model_path,
    limit_mm_per_prompt={"image": 10, "video": 10},
    dtype=torch.float16,
)

sampling_params = SamplingParams(
    temperature=0.1,
    top_p=0.001,
    repetition_penalty=1.05,
    max_tokens=256,
    stop_token_ids=[],
)

# Load the MathVista dataset
dataset = load_dataset("AI4Math/MathVista")

subset_dataset = dataset["testmini"].select(range(100))
# Prepare results storage
responses = {}

# Batch processing parameters
batch_size = 8  # Adjust based on GPU memory and model size

# Start timing
start_time = time.time()

# Initialize processor outside the loop for efficiency
processor = AutoProcessor.from_pretrained(model_path)

# Split dataset into batches
for i in range(0, len(subset_dataset), batch_size):
    batch = subset_dataset.select(range(i, min(i + batch_size, len(subset_dataset))))
    prompts = []
    mm_data_list = []

    # Prepare prompts and multimodal data for the batch
    for example in batch:
        try:
            query = example["query"]
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": example["decoded_image"]},
                        {"type": "text", "text": query},
                    ],
                }
            ]

            prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, _ = process_vision_info(messages)

            mm_data = {}
            if image_inputs is not None:
                mm_data["image"] = image_inputs

            prompts.append(prompt)
            mm_data_list.append(mm_data)
        except KeyError as e:
            print(f"Missing key in example: {e}. Skipping this example.")
            continue

    # Skip batch if no valid prompts
    if not prompts:
        print("No valid prompts in this batch. Skipping batch.")
        continue

    # Batch inference
    outputs = llm.generate(
        [
            {"prompt": prompt, "multi_modal_data": mm_data}
            for prompt, mm_data in zip(prompts, mm_data_list)
        ],
        sampling_params=sampling_params
    )

    # Process the results for the batch
    for idx, example in enumerate(batch):
        try:
            response = outputs[idx].outputs[0].text
            responses[example["pid"]] = {
                "query": example["query"],
                "response": response,
                "question_type": example["question_type"],
                "answer_type": example["answer_type"],
                "choices": example.get("choices"),
                "precision": example.get("precision"),
                "answer": example["answer"],
            }
        except (KeyError, IndexError) as e:
            print(f"Error processing example: {e}. Skipping this result.")
            continue

# End timing
time_cost = time.time() - start_time
print(f"Time cost for generating responses: {time_cost:.2f} seconds")

# Save responses to a file
responses_file = os.path.join(output_dir, "responses_batch.json")
with open(responses_file, "w") as f:
    json.dump(responses, f)

print(f"Response generation completed. Saved to {responses_file}.")

