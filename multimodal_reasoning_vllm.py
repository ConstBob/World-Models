# -*- coding: utf-8 -*-
"""Multimodal_Reasoning_vLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dm_yExDKSfzNhjiim2DuOFljEsKdDPFM
"""

!nvcc --version

# !pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830
!pip install git+https://github.com/huggingface/transformers
!pip install accelerate
!pip install qwen-vl-utils
!pip install datasets
!CUDA_VERSION=cu121
!pip install 'vllm==0.6.1' --extra-index-url https://download.pytorch.org/whl/${CUDA_VERSION}

from google.colab import drive
import os
import json
import re
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from datasets import load_dataset

# Mount Google Drive
drive.mount('/content/drive')

# Create necessary directories in Google Drive
output_dir = "/content/drive/My Drive/World Models"
os.makedirs(output_dir, exist_ok=True)

# Use GPU
if torch.cuda.is_available():
    print("GPU is available and ready to use!")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
else:
    print("GPU is not available. Make sure it's enabled in the runtime settings.")

# Response Generation with vLLM
from vllm import LLM, SamplingParams
from transformers import AutoProcessor
from qwen_vl_utils import process_vision_info

model_save_path = os.path.join(output_dir, "qwen_model")
if os.path.exists(model_save_path):
    print("Loading model from local save path.")
    model_path = model_save_path
else:
    print("Downloading model. This may take some time...")
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        "Qwen/Qwen2-VL-2B-Instruct", torch_dtype="auto", device_map="auto"
    )
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
    model.save_pretrained(model_save_path)
    processor.save_pretrained(model_save_path)

print(model_save_path)
print(model_path)

from transformers import AutoConfig

config = AutoConfig.from_pretrained(model_path)
if not hasattr(config, "rope_scaling") or "factor" not in config.rope_scaling:
    config.rope_scaling = {"type": "linear", "factor": 1.0}  # Default
if not hasattr(config, "rope_type"):
    config.rope_type = "yarn"  # Default
if not hasattr(config, "max_model_len"):
    config.max_model_len = 2048  # Adjust per the model

config.save_pretrained(model_path)

# from transformers import AutoTokenizer
# model_name = "Qwen/Qwen2-VL-2B-Instruct"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# tokenizer.save_pretrained(model_path)

llm = LLM(
    model=model_path,
    limit_mm_per_prompt={"image": 10, "video": 10},
    dtype=torch.float16,
)

sampling_params = SamplingParams(
    temperature=0.1,
    top_p=0.001,
    repetition_penalty=1.05,
    max_tokens=256,
    stop_token_ids=[],
)

# Load the MathVista dataset
dataset = load_dataset("AI4Math/MathVista")

subset_dataset = dataset["testmini"]

# Prepare results storage
responses = {}

import time
start_time = time.time()

# Generate responses
for idx, example in enumerate(subset_dataset):

    query = example["query"]  # Use the query field directly from the dataset
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": example["decoded_image"]},
                {"type": "text", "text": query},
            ],
        }
    ]
    processor = AutoProcessor.from_pretrained(model_path)
    prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, _ = process_vision_info(messages)

    mm_data = {}
    if image_inputs is not None:
        mm_data["image"] = image_inputs

    llm_inputs = {
        "prompt": prompt,
        "multi_modal_data": mm_data,
    }

    outputs = llm.generate([llm_inputs], sampling_params=sampling_params)
    response = outputs[0].outputs[0].text
    # response_cleaned = response.split("\nassistant\n")[-1].strip()

    responses[example["pid"]] = {
        "query": query,
        "response": response,
        "question_type": example["question_type"],
        "answer_type": example["answer_type"],
        "choices": example.get("choices"),
        "precision": example.get("precision"),
        "answer": example["answer"],
    }



time_cost = time.time() - start_time
print(f"Time cost for generating responses: {time_cost:.2f} seconds")

responses_file = os.path.join(output_dir, "responses.json")
with open(responses_file, "w") as f:
    json.dump(responses, f)

print(f"Response generation completed. Saved to {responses_file}.")

